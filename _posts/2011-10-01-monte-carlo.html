---
layout: post
title: "Monte Carlo Methods for Managing Interactive State, Action and Feedback Under Uncertainty"
category: publications
icon: "assets/cards/monte-carlo2.png"
authors: "Julia Schwarz, Jennifer Mankoff, Scott E. Hudson, Andy Wilson"
proceedings: "In Proceedings of UIST '11"
excerpt: "The second of two papers presenting a radically new input dispatch system for handling inputs with uncertainty (e.g. voice, touch, gesture).
Tracks the likelihood of interface alternatives by deterministically simulating hundreds of input alternatives and aggregating results.
This allows develpers to build uncertain interfaces without having to think probabilistically."
paper: "assets/monte-carlo/schwarz-uist11-montecarlo.pdf"
demo_video: "https://www.youtube.com/watch?v=vW15T1wDcGQ"
header_image: "assets/monte-carlo/monte-carlo-banner2.png"
citation: "Schwarz, J., Mankoff, J., Hudson, S., Monte Carlo Methods for Managing Interactive State, Action and Feedback Under Uncertainty. In Proceedings of 24th Annual ACM Symposium on User Interface Software and Technology (Santa Barbara, California, October 16-19, 2011). UIST â€™11. ACM, New York, NY, 235 - 244."
---
<div class="row">
    <div class="col-md-6">
        <p><em>Normally, for these explanations I start with a paper summary. But the topic of uncertain user
            interfaces requires a bit more context, so first I will provide some background.
        </em></p>
        <p>
            Current input handling systems provide effective techniques for modeling, tracking, interpreting,
            and acting on user input. However, new interaction technologies violate the standard assumption that
            input is certain. Touch, speech recognition, gestural input, and sensors for context often produce
            uncertain estimates of user inputs. Current systems tend to remove uncertainty early on (e.g. taking the most likely input), trying to fit
            these uncertain user inputs into the cookie-cutter formulations of mice and keyboards.
            Because these uncertain inputs are error-prone, we are forced to adapt our interfaces and behaviors to adjust to these
            new input mechanisms by making larger buttons, speaking slowly, and carefully executing gestures.
        </p>
        <p>
            I believe that we shouldn't have to adapt our behaviors to suit our interfaces, and that instead interfaces should
            adapt to us, taking into account information about our past behaviors, our preferences, and other contextual information to
            make more intelligent predictions about our intended actions. Furthermore, I believe that in order to achieve this,
            we need to fundamentally change the way our user interface systems are built to more easily allow for
            incorporating likelihoods, context, user behavior, and predictive models.
        </p>
        <p>
            To this end, I'm working on designing
            a new architecture for user interface toolkits which tracks the likelihoods of alternate input interpretations and
            interface states for as long as possible before making a decision. A central goal of my work is
            to keep the programming model for developers simple: developers shouldn't have to think probabilistically to use
            the toolkit, but should be able to take advantage of incorporating context and user behavior.
        </p>
    </div>
    <div class="col-md-6">
        <iframe width="420" height="315" class="center-block img-responsive" style="height: 315px;" src="//www.youtube.com/embed/vW15T1wDcGQ?rel=0" frameborder="0" allowfullscreen></iframe>
        <div class="figure_caption top-spacing ">
            <small>This video summary contains a demonstration of the capabilities of the framework as well as an explanation of the architecture. </small>
        </div>
    </div>
</div>
    <div class="row">
        <div class="col-md-6">
            <h3>Summary</h3>
            <p>
                One challenge I ran into when implementing the demos for my <a href="{{site.baseurl}}/publications/2010/10/02/probabilistic-input/">
                first paper</a> was that I found it difficult to
                track that state of any user interface element that had more than two states. Because the input events were probabilistic,
                it became difficult to manually track the likelihood of different states, especially as the state machines
                became more complex. The aim of this paper was to extend and modify the initial architecture to
                make it easy for developers to track the probabilistic state of user interface elements.
                The key insight in this paper was to use  a Monte Carlo approach to maintain a
                probabilistically accurate description of the user interface that can be used to make informed choices about
                actions. Samples are used to approximate the distribution of possible inputs, possible interactor states that
                result from inputs, and possible actions (callbacks and feedback) interactors may execute.
            </p>
            <p>
                Because each sample is certain, the developer can specify most of the behavior of interactors in a familiar,
                non-probabilistic fashion. This approach retains all the advantages of maintaining information about uncertainty
                while minimizing the need for the developer to work in probabilistic terms. This paper presents a working implementation
                of our framework and illustrates the power of these techniques within a paint program that includes three
                different kinds of uncertain input.
            </p>
            <p>
                This is the second of three research papers (one is not yet published) that aim to lay
                the foundations for a radically new user interface architecture built to handle uncertain user inputs. The
                image above presents a diagram aiming to explain the architecture, however for a deeper understanding please
                see the <a href="{{site.baseurl}}/assets/monte-carlo/schwarz-uist11-montecarlo.pdf">paper</a>.
            </p>
        </div>
        <div class="col-md-6">
            <img class="img-responsive center-block" src="{{site.baseurl}}/assets/monte-carlo/finger-gaussian-samples.png">
            <div class="figure_caption top-spacing bottom-spacing ">
                <small>A touch event is often represented as a single point, but in fact the finger is touching an entire area.
                 My system approximates this distribution by generating a collection of individual touch events (black dots)
                which are dispatched in parralel and results aggregated.</small>
            </div>
        </div>
    </div>


