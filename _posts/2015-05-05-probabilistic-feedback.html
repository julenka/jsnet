---
layout: post
title: "An Architecture for Generating Interactive Feedback in Probabilistic User Interfaces"
category: publications
icon: "assets/cards/probabilistic-feedback.jpg"
authors: "Julia Schwarz, Jennifer Mankoff, Scott E. Hudson"
proceedings: "In Proceedings of CHI '15, Honorable Mention for Best Paper"
excerpt: "A probabilistic user interface architecture that not tracks probabilistic UI state, but communicates this uncertainty
back to the user in a meaningful way.

This is accomplished by tracking possible interfaces, reducing the number of possible interfaces to a few exemplars, and fusing exemplars to communicate uncertainty and allow for disambiguation."
paper: "assets/probabilistic-feedback/schwarz-chi15-probabilisticfeedback.pdf"
demo_video: "http://youtu.be/1SzLxofVWV8"
citation: "Schwarz, J., Mankoff, J., Hudson, S., An Architecture for Generating Interactive Feedback in Probabilistic User Interfaces. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (Seoul, Korea, April 18 - 23, 2015). CHI ’15. ACM, New York, NY, 2545 - 2554."
---
<div class="row">
    <div class="col-md-6">
        <p><em>Normally I start with a paper summary. But the topic of uncertain user
            interfaces requires a bit more context, so first some background.
        </em></p>
        <p> <em>
            Current input handling systems provide effective techniques for modeling, tracking, interpreting,
            and acting on user input. However, new interaction technologies violate the standard assumption that
            input is certain. Touch, speech recognition, and gestural input often produce
            uncertain estimates of user inputs. Current systems tend to remove uncertainty early on by using 
            the most likely sensed input (e.g. taking the center of the touch area). As a result, we are 
            forced to adapt by making larger buttons, speaking slowly, and carefully executing gestures.
            </em>
        </p>
        <p>
            <em>
            I believe that we shouldn't have to adapt our behaviors to suit our interfaces, and that instead interfaces should
            adapt to us, taking into account information about our past behaviors and our preferences to
            make better predictions. To achieve this
            we need to fundamentally change the way our user interface systems are built. Our user interface systems
            need to incorporate probabilistic reasoning.
            </em>
        </p>
        <p>
            <em>
            For my PhD thesis I developed
            a new architecture for user interface toolkits which incorporates probabilistic reasoning.
            It tracks the likelihoods of alternate input interpretations and
            interface states for as long as possible before making a decision.
            </em>
        </p>
        <p>
            <em>
                This paper is the last of three research papers published as part of my thesis.
            </em>
        </p>
    </div>
    <div class="col-md-6">
            <img class="img-responsive center-block" 
                src="{{site.baseurl}}/assets/probabilistic-feedback/nbest_gesture_420.gif">

            <div class="top-spacing figure_caption bottom-spacing">
                <small>Auto-correct. As I draw letters, a gesture recognizer tries to 
                guess which shapes I drew. I can correct the recognizer by selecting an alternative. </small>
            </div>        
    </div>  
    
</div>
    <div class="row">
        <div class="col-md-6">
            <h3>Summary</h3>
            <p>
                Increasingly natural, sensed, and touch-based input is being integrated into devices. Along the way, both custom and
                more general solutions have been developed for dealing with the uncertainty that is associated with these forms of
                input. However, it is difficult to provide dynamic, flexible, and continuous feedback about uncertainty using traditional
                interactive infrastructure. Our contribution is a general architecture with the goal of providing support for continual
                feedback about uncertainty.
            </p>
            <p>
                Our architecture is based on prior work in modeling uncertainty using Monte Carlo sampling, and tracks multiple
                interfaces – one for each plausible and differentiable sequence of input that the user may have intended. This paper
                presents a method for reducing the number of alternative interfaces and fusing possible interfaces into a single
                interface that both communicates uncertainty and allows for disambiguation. We demonstrate the value of this result
                through a collection of 11 new and existing feedback techniques along with two applications demonstrating the use of
                the feedback architecture.
            </p>
        </div>
    <div class="col-md-6">
        <img class="img-responsive center-block" 
                src="{{site.baseurl}}/assets/probabilistic-feedback/snap_480p.gif">

            <div class="top-spacing figure_caption bottom-spacing">
                <small> Smart snap. As I draw lines and shapes, the app predicts possible targets to snap to
                and gives me the option to choose a prediction. Of course, I can ignore these suggestions if I wish. </small>
            </div>  
    </div>
</div>
