---
layout: post
title: "A Robust and Flexible Framework for Handling Inputs with Uncertainty"
category: publications
icon: "assets/cards/prob-input.jpg"
authors: "Julia Schwarz, Scott E. Hudson, and Jennifer Mankoff"
proceedings: "In Proceedings of UIST '10"
excerpt: "The first of two papers presenting a radically new input dispatch system for handling inputs with uncertainty (e.g. voice, touch, gesture).
Presents a general input dispatch architecture where interface elements are queried regarding their suitability to handle an event."
demo_video: "http://youtu.be/R9hRj2dUky8"
paper: "assets/prob-input/schwarz-uist10-prob-input.pdf"
header_image: "assets/prob-input/prob-input-banner.jpeg"
citation: "Schwarz, J., Hudson, S., Mankoff, J., A Robust and Flexible Framework for Handling Inputs with Uncertainty. In Proceedings of the 23rd Annual ACM Symposium on User Interface Software and Technology (New York, New York, October 3 – 6, 2010). UIST’10. ACM, New York, NY, 47 - 56."
---
<div class="row">
    <div class="col-md-6">
        <p><em>Normally, for these explanations I start with a paper summary. But the topic of uncertain user
            interfaces requires a bit more context, so first I will provide some background.
        </em></p>
        <p>
            Current input handling systems provide effective techniques for modeling, tracking, interpreting,
            and acting on user input. However, new interaction technologies violate the standard assumption that
            input is certain. Touch, speech recognition, gestural input, and sensors for context often produce
            uncertain estimates of user inputs. Current systems tend to remove uncertainty early on (e.g. taking the most likely input), trying to fit
            these uncertain user inputs into the cookie-cutter formulations of mice and keyboards.
            Because these uncertain inputs are error-prone, we are forced to adapt our interfaces and behaviors to adjust to these
            new input mechanisms by making larger buttons, speaking slowly, and carefully executing gestures.
        </p>
        <p>
            I believe that we shouldn't have to adapt our behaviors to suit our interfaces, and that instead interfaces should
            adapt to us, taking into account information about our past behaviors, our preferences, and other contextual information to
            make more intelligent predictions about our intended actions. Furthermore, I believe that in order to achieve this,
            we need to fundamentally change the way our user interface systems are built to more easily allow for
            incorporating likelihoods, context, user behavior, and predictive models.
        </p>
        <p>
            To this end, I'm working on designing
            a new architecture for user interface toolkits which tracks the likelihoods of alternate input interpretations and
            interface states for as long as possible before making a decision. A central goal of my work is
            to keep the programming model for developers simple: developers shouldn't have to think probabilistically to use
            the toolkit, but should be able to take advantage of incorporating context and user behavior.
        </p>
    </div>
    <div class="col-md-6">
        <iframe width="420" height="315" class="center-block img-responsive" style="height: 315px;" src="//www.youtube.com/embed/R9hRj2dUky8?rel=0" frameborder="0" allowfullscreen></iframe>
        <div class="figure_caption top-spacing ">
            <small>This video summary contains a demonstration of the capabilities of the framework as well as an explanation of the architecture. </small>
        </div>
    </div>
</div>
<div class="row">
    <div class="col-md-6">
        <h3>Summary</h3>
        <p>
            This paper presents a framework for handling input with uncertainty in a systematic, extensible, and easy to manipulate
            fashion. To illustrate this framework, the paper presents several traditional interactors which have been extended to
            provide feedback about uncertain inputs and to allow for the possibility that in the end that input will be
            judged wrong (or end up going to a different interactor). The six demonstrations in the paper include tiny buttons that are
            manipulable using touch input, a text box that can handle multiple interpretations of spoken input, a scrollbar
            that can respond to inexactly placed input, and buttons which are easier to click for people with motor impairments.
        </p>
        <p>
            This initial framework supports all of these interactions by carrying uncertainty forward all the way through selection of
            possible target interactors, interpretation by interactors, generation of (uncertain) candidate actions to take,
            and a mediation process that decides (in a lazy fashion) which actions should become final.
        </p>
        <p>
            This is the first of three research papers (one is not yet published) that aim to lay foundations of
            a radically new user interface architecture built to handle uncertain user inputs. It represents the
            initial version of a user interface architecture for handling uncertain user inputs. <a href="{{site.baseurl}}/publications/2011/10/01/monte-carlo/">Subsequent revisions
            </a>
            draw from ideas presented in this framework. The
            image above presents a diagram aiming to explain the architecture, however for a deeper understanding please
            see the <a href="{{site.baseurl}}/{{page.paper}}">paper</a>.
        </p>
    </div>
    <div class="col-md-6">
        <div class="bottom-spacing top-spacing"></div>
        <img class="img-responsive center-block" src="{{site.baseurl}}/assets/prob-input/fig1.png">
        <div class="figure_caption top-spacing bottom-spacing ">
            <small>Illustration of implicit disambiguation for touch input.
                Left: Users presses down and moves diagonally, moving the icon.
                Right: User presses down and moves horizontally, resizing the window instead.
                When the user presses down, both interactors are equally likely to respond.
                The user’s motion later disambiguates their intention and interactors in our framework respond appropriately.</small>
        </div>
        <h3>Acknowledgements</h3>
        Special thanks to <a href="http://www.christianholz.net/" target="_blank">Christian Holz</a> for providing the <a href="http://www.christianholz.net/generalized_perceived_input_point_model.html" target="_blank">original artwork</a> used in the homepage icon for this project.
    </div>
</div>